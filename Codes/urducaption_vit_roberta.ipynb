{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import pandas as pd\nfile = open('../input/urdu-flick8k-caps/KA_dogs.urd.descriptionsManual29Feb20.txt','r', encoding='utf-8-sig')\ntext = file.read()\nfile.close()\n\ndatatxt = []\n\nfor line in text.split('\\n'):\n    col = line.split()\n    # print(col)\n    if len(col) <2:\n        continue\n    w = col[0]\n#     print(\" \".join(col[1:]))\n    if(w!='3558251719_3af5ae2d02ایک' and w!='430173345_86388d8282'and w!='2258277193_586949ec62'):\n        datatxt.append([w+\".jpg\"] + [\" \".join(col[1:])])\n# print(datatxt)\n\ndata = pd.DataFrame(datatxt,columns=[\"images\",\"text\"])\ndata\n# data = data.reindex(columns =['images','text'])\n# data = data[data['images'] != '2258277193_586949ec62.jpg.1']","metadata":{"execution":{"iopub.status.busy":"2022-08-06T09:07:13.972340Z","iopub.execute_input":"2022-08-06T09:07:13.972749Z","iopub.status.idle":"2022-08-06T09:07:14.058049Z","shell.execute_reply.started":"2022-08-06T09:07:13.972676Z","shell.execute_reply":"2022-08-06T09:07:14.057183Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"# data['images'][7040]\nlen(data)","metadata":{"execution":{"iopub.status.busy":"2022-08-06T09:07:14.059582Z","iopub.execute_input":"2022-08-06T09:07:14.060007Z","iopub.status.idle":"2022-08-06T09:07:14.066567Z","shell.execute_reply.started":"2022-08-06T09:07:14.059970Z","shell.execute_reply":"2022-08-06T09:07:14.065733Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"import torch\nfrom PIL import Image\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import Dataset\n\n\nclass Image_Caption_Dataset(Dataset):\n    def __init__(\n        self, root_dir, df, feature_extractor, tokenizer, max_target_length=512\n    ):\n        self.root_dir = root_dir\n        self.df = df\n        self.feature_extractor = feature_extractor\n        self.tokenizer = tokenizer\n        self.max_length = max_target_length\n\n    def __len__(self):\n        return self.df.shape[0]\n\n    def __getitem__(self, idx):\n        # return image\n        image_path = self.df[\"images\"][idx] \n        text = self.df[\"text\"][idx]\n#         print(image_path)\n#         print(text)\n#         print(\"Index\",idx)\n        # prepare image\n        image = Image.open(self.root_dir + \"/\" + image_path).convert(\"RGB\")\n        pixel_values = self.feature_extractor(image, return_tensors=\"pt\").pixel_values\n        # add captions by encoding the input\n        captions = self.tokenizer(\n            text, padding=\"max_length\", max_length=self.max_length\n        ).input_ids\n        captions = [\n            caption if caption != self.tokenizer.pad_token_id else -100\n            for caption in captions\n        ]\n        encoding = {\n            \"pixel_values\": pixel_values.squeeze(),\n            \"labels\": torch.tensor(captions),\n        }\n        return encoding\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-08-06T09:07:15.627343Z","iopub.execute_input":"2022-08-06T09:07:15.627824Z","iopub.status.idle":"2022-08-06T09:07:18.149157Z","shell.execute_reply.started":"2022-08-06T09:07:15.627775Z","shell.execute_reply":"2022-08-06T09:07:18.148186Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"train_df, val_df = train_test_split(data, test_size=0.2, random_state=42)\n    \ndef load_dataset(root_dir, df, feature_extractor, tokenizer, max_target_length=512):\n    # split the dataset into train and test\n    \n\n    train_df.reset_index(inplace=True)\n    val_df.reset_index(inplace=True)\n    train_dataset = Image_Caption_Dataset(\n        root_dir, train_df, feature_extractor, tokenizer, max_target_length\n    )\n    val_dataset = Image_Caption_Dataset(\n        root_dir, val_df, feature_extractor, tokenizer, max_target_length\n    )\n    return train_dataset, val_dataset\n","metadata":{"execution":{"iopub.status.busy":"2022-08-06T09:07:19.909618Z","iopub.execute_input":"2022-08-06T09:07:19.910143Z","iopub.status.idle":"2022-08-06T09:07:19.921363Z","shell.execute_reply.started":"2022-08-06T09:07:19.910092Z","shell.execute_reply":"2022-08-06T09:07:19.920258Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\nfrom transformers.utils import logging\nfrom transformers import (AutoTokenizer,AutoModelForMaskedLM, Seq2SeqTrainer,\n                          Seq2SeqTrainingArguments, VisionEncoderDecoderModel,\n                          ViTFeatureExtractor, default_data_collator)\nlogging.set_verbosity_info()\n# logging.set_verbosity_error()\nlogger = logging.get_logger(\"transformers\")\n# logger.info(\"INFO\")\n# logger.warning(\"WARN\")\nimport os\nos.environ[\"WANDB_DISABLED\"] = \"true\"\ncaptions_path = \"../input/urdu-flick8k-caps/KA_dogs.urd.descriptionsManual29Feb20.txt\"\nroot_dir = \"../input/flicker8k/data/Images\"\n\nencoder_checkpoint = \"google/vit-base-patch16-224\"\ndecoder_checkpoint = \"urduhack/roberta-urdu-small\"\noutput_dir = \"./image_captioning_checkpoint\"\n# load feature extractor and tokenizer\nfeature_extractor = ViTFeatureExtractor.from_pretrained(encoder_checkpoint)\ntokenizer = AutoTokenizer.from_pretrained(decoder_checkpoint)\n\n\n\ndata\n\n\ntrain_dataset, val_dataset = load_dataset(root_dir,data,feature_extractor, tokenizer)\nprint(len(train_dataset),len(val_dataset))\n\n# initialize a vit-bert from a pretrained ViT and a pretrained GPT2 model\nmodel = VisionEncoderDecoderModel.from_encoder_decoder_pretrained(\n    encoder_checkpoint, decoder_checkpoint\n)\n# set special tokens used for creating the decoder_input_ids from the labels\nmodel.config.decoder_start_token_id = tokenizer.bos_token_id\nmodel.config.pad_token_id = tokenizer.pad_token_id\n# make sure vocab size is set correctly\nmodel.config.vocab_size = model.config.decoder.vocab_size\n\n# set beam search parameters\nmodel.config.eos_token_id = tokenizer.sep_token_id\nmodel.config.max_length = 512\nmodel.config.early_stopping = True\nmodel.config.no_repeat_ngram_size = 1\nmodel.config.length_penalty = 2.0\nmodel.config.num_beams = 4\nmodel.decoder.resize_token_embeddings(len(tokenizer))\n\n# freeze the encoder\nfor param in model.encoder.parameters():\n    param.requires_grad = False\n# predict_with_generate=True,\n#     evaluation_strategy=\"epoch\",\n#     per_device_train_batch_size=8,\n#     per_device_eval_batch_size=8,\n#     overwrite_output_dir=True,\n#     fp16=True,\n#     save_strategy='epoch',\n#     run_name=\"15_epoch_run\",\n#     load_best_model_at_end=True,\n#     output_dir=output_dir,\n#     logging_steps=2000,\n#     save_steps=2000,\n#     eval_steps=2000,\n#     num_train_epochs=4,\n#     push_to_hub=True,\n#     hub_model_id=\"Vit_roberta_urdu\",\n#     hub_strategy=\"every_save\",\n#     hub_token=\"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\"\n\ntraining_args = Seq2SeqTrainingArguments(\n    predict_with_generate=True,\n    evaluation_strategy=\"steps\",\n    per_device_train_batch_size=8,\n    per_device_eval_batch_size=8,\n    overwrite_output_dir=True,\n    fp16=True,\n    run_name=\"best_run\",\n    load_best_model_at_end=True,\n    output_dir=output_dir,\n    logging_steps=2000,\n    save_steps=2000,\n    eval_steps=2000,\n#     push_to_hub=True\n    \n)\n\n\nif __name__ == \"__main__\":\n    # instantiate trainer\n    trainer = Seq2SeqTrainer(\n        model=model,\n        tokenizer=feature_extractor,\n        args=training_args,\n        train_dataset=train_dataset,\n        eval_dataset=val_dataset,\n        data_collator=default_data_collator,\n#         optim=\"adamw_torch\"\n    )\n    trainer.train()\n","metadata":{"execution":{"iopub.status.busy":"2022-08-06T09:07:20.625881Z","iopub.execute_input":"2022-08-06T09:07:20.626265Z","iopub.status.idle":"2022-08-06T09:50:37.786440Z","shell.execute_reply.started":"2022-08-06T09:07:20.626234Z","shell.execute_reply":"2022-08-06T09:50:37.785604Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# !apt-get install git-lfs\n# !git lfs install\n# trainer.save_model()\n# tokenizer.save_model()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import os\n# li=os.listdir('../input/flicker8k/data/Images')\n# for i in data['images']:\n#     if i not in li:\n#         print(i)\n# len(train_dataset), len(val_dataset)\n# train_dataset\nval_dataset","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\ntorch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2022-08-06T09:50:44.760683Z","iopub.execute_input":"2022-08-06T09:50:44.761484Z","iopub.status.idle":"2022-08-06T09:50:45.009030Z","shell.execute_reply.started":"2022-08-06T09:50:44.761444Z","shell.execute_reply":"2022-08-06T09:50:45.008169Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"import requests\nimport torch\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nif torch.cuda.is_available():\n    device = \"cuda\"\nelse:\n    device = \"cpu\"\n\nurl = \"https://i.pinimg.com/564x/e6/b2/b0/e6b2b0c15a16221ec5a806362504047c.jpg\"\nimage = Image.open(requests.get(url, stream=True).raw)\nplt.imshow(image)\nplt.show()\ndef predict(image,beams=3):\n    clean_text = lambda x: x.replace(\"<|endoftext|>\", \"\").split(\"\\n\")[0]\n    sample = feature_extractor(image, return_tensors=\"pt\").pixel_values.to(device)\n    caption_ids = model.generate(sample, max_length=50,repetition_penalty=1.5,num_beams = beams,early_stopping = True,no_repeat_ngram_size = 2)[0]\n    caption_text = clean_text(tokenizer.decode(caption_ids))\n    return caption_text\npredict(image)\n# max_length = MAX_LEN, \n#     num_beams = 5, \n#     no_repeat_ngram_size = 2, \n#     num_return_sequences = 5, \n#     early_stopping = True","metadata":{"execution":{"iopub.status.busy":"2022-08-06T10:22:59.756159Z","iopub.execute_input":"2022-08-06T10:22:59.756840Z","iopub.status.idle":"2022-08-06T10:23:01.277813Z","shell.execute_reply.started":"2022-08-06T10:22:59.756803Z","shell.execute_reply":"2022-08-06T10:23:01.276866Z"},"trusted":true},"execution_count":55,"outputs":[]},{"cell_type":"code","source":"# with open(captions_path,encoding=\"utf8\") as f:\n#     data = []\n\n#     for i in f.readlines():\n#         sp = i.split(\" \")\n#         data.append([sp[0] + \".jpg\", \" \".join(sp[1:])])\n\n# data = pd.DataFrame(data, columns=[\"images\", \"text\"])\nimage = Image.open(root_dir + \"/\" + data['images'][0]).convert(\"RGB\")\nplt.imshow(image)\nplt.show()\npredict(image)\n","metadata":{"execution":{"iopub.status.busy":"2022-08-06T09:50:57.245377Z","iopub.execute_input":"2022-08-06T09:50:57.246290Z","iopub.status.idle":"2022-08-06T09:50:57.704249Z","shell.execute_reply.started":"2022-08-06T09:50:57.246244Z","shell.execute_reply":"2022-08-06T09:50:57.703489Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nfrom nltk.translate.bleu_score import sentence_bleu\n# select random image from validation data\nrid = np.random.randint(0, len(data))\nimage = data['images'][rid]\nreal_caption = data['text'][rid]\nprint(\"Real:\",image,real_caption)\n# caption,result,attention_weights = evaluate(image)\nimage = Image.open(root_dir + \"/\" + data['images'][rid]).convert(\"RGB\")\nplt.imshow(image)\nplt.show()\ncaption=predict(image)\ncaption=caption.replace(\"<s>\",\"\")\ncaption=caption.replace(\"</s>\",\"\")\ncaption=caption.rstrip()\n\nfirst = real_caption\nreal_caption = first\n\n\n#remove \"<unk>\" in result\nfor i in caption:\n    if i==\"<unk>\":\n        caption.remove(i)\n\nfor i in real_caption:\n    if i==\"<unk>\":\n        real_caption.remove(i)\n\n\n\nreal_appn = []\nreal_appn.append(real_caption.split())\nreference = real_appn\ncandidate = caption.split()\nprint(reference,candidate)\nscore = sentence_bleu(reference, candidate, weights=(1.0,0,0,0))\nprint(f\"BLEU-1 score: {score*100}\")\nscore = sentence_bleu(reference, candidate, weights=(0.5,0.5,0,0))\nprint(f\"BLEU-2 score: {score*100}\")\nscore = sentence_bleu(reference, candidate, weights=(0.3,0.3,0.3,0))\nprint(f\"BLEU-3 score: {score*100}\")\nscore = sentence_bleu(reference, candidate, weights=(0.25,0.25,0.25,0.25))\nprint(f\"BLEU-4 score: {score*100}\")\n\n# print ('Real Caption:', real_caption)\n# print ('Predicted Caption:', ' '.join(caption))\n# temp_image = np.array(Image.open(image))\n# plt.imshow(temp_image)\n","metadata":{"execution":{"iopub.status.busy":"2022-08-06T09:51:19.296990Z","iopub.execute_input":"2022-08-06T09:51:19.297581Z","iopub.status.idle":"2022-08-06T09:51:20.341324Z","shell.execute_reply.started":"2022-08-06T09:51:19.297545Z","shell.execute_reply":"2022-08-06T09:51:20.340429Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\n# import pandas as pd\n# from transformers.utils import logging\n# from transformers import (AutoTokenizer,AutoModelForMaskedLM, Seq2SeqTrainer,\n#                           Seq2SeqTrainingArguments, VisionEncoderDecoderModel,\n#                           ViTFeatureExtractor, default_data_collator)\n# encoder_checkpoint = \"google/vit-base-patch16-224\"\n# decoder_checkpoint = \"urduhack/roberta-urdu-small\"\n# output_dir = \"./image_captioning_checkpoint\"\n# # load feature extractor and tokenizer\n# feature_extractor = ViTFeatureExtractor.from_pretrained(encoder_checkpoint)\n# tokenizer = AutoTokenizer.from_pretrained(decoder_checkpoint)\n\n\n# model_checkpoint = \"./image_captioning_checkpoint\"\n# model = VisionEncoderDecoderModel.from_pretrained(model_checkpoint).to(device)\n\n\n# # initialize a vit-bert from a pretrained ViT and a pretrained GPT2 model\n# model = VisionEncoderDecoderModel.from_encoder_decoder_pretrained(\n#     encoder_checkpoint, decoder_checkpoint\n# )","metadata":{"execution":{"iopub.status.busy":"2022-08-06T09:51:31.865881Z","iopub.execute_input":"2022-08-06T09:51:31.866355Z","iopub.status.idle":"2022-08-06T09:51:31.870905Z","shell.execute_reply.started":"2022-08-06T09:51:31.866312Z","shell.execute_reply":"2022-08-06T09:51:31.869952Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"from huggingface_hub import notebook_login\n\nnotebook_login()","metadata":{"execution":{"iopub.status.busy":"2022-06-24T05:01:46.652452Z","iopub.execute_input":"2022-06-24T05:01:46.652899Z","iopub.status.idle":"2022-06-24T05:01:46.746571Z","shell.execute_reply.started":"2022-06-24T05:01:46.652863Z","shell.execute_reply":"2022-06-24T05:01:46.745446Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2022-07-03T07:45:46.493089Z","iopub.execute_input":"2022-07-03T07:45:46.493822Z","iopub.status.idle":"2022-07-03T07:45:46.503722Z","shell.execute_reply.started":"2022-07-03T07:45:46.493782Z","shell.execute_reply":"2022-07-03T07:45:46.502969Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#BERT Score\n!pip install bert_score\n# !cp '/gdrive/My Drive/Colab Notebooks/ImageCaptionThesis/utils.py' /usr/local/lib/python3.6/dist-packages/bert_score/utils.py\nfrom bert_score import score\ncands =[\n     'دو کتے  پانی میں کھیل رہے ہیں',\n     'پانی میں دو کتے بھاگ  رہے ہیں',\n     'دو کتے جنگل میں  بھاگ  رہے ہیں']\nrefs = [  ['ایک کتا پانی میں بھاگ رہا ہے'],['ایک کتا پانی میں بھاگ رہا ہے'],['ایک کتا پانی میں بھاگ رہا ہے']]\nP, R, F1 = score(cands, refs, lang=\"ur\", verbose=False)\nP, R, F1","metadata":{"execution":{"iopub.status.busy":"2022-08-06T09:51:36.603505Z","iopub.execute_input":"2022-08-06T09:51:36.603859Z","iopub.status.idle":"2022-08-06T09:52:41.125974Z","shell.execute_reply.started":"2022-08-06T09:51:36.603829Z","shell.execute_reply":"2022-08-06T09:52:41.125000Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"# LASER Score\n!pip install laserembeddings\n!python -m laserembeddings download-models\nfrom laserembeddings import Laser\nlaser = Laser()\n\n# if all sentences are in the same language:\n\nembeddings = laser.embed_sentences(\n    ['ایک کتا پانی میں بھاگ رہا ہے',\n     'دو کتے  پانی میں کھیل رہے ہیں',\n     'پانی میں دو کتے بھاگ  رہے ہیں',\n     'دو کتے جنگل میں  بھاگ  رہے ہیں'],\n    lang='ur')  # lang is only used for tokenization\n\nimport math\ndef square_rooted(x):\n    return math.sqrt(sum([a*a for a in x]))\n\ndef cosine_similarity(x,y):\n    numerator = sum(a*b for a,b in zip(x,y))\n    denominator = square_rooted(x)*square_rooted(y)\n    return ( numerator,float(denominator) ) # Can do Micro-Macro Anvging here by returning the numerator and denominator only. Where the caller can progressivly compute both MacroAvg:CosSum=CosSum+Num/Denum, count+1; finally CosSum/Count\n                                                                                                                                                    # MicroAvg: NumrtrSum=NumrtrSum+Numrtr,denomSum=denomSum+Denom; finally NumrtrSum/denomSum\ncosine_similarity(embeddings[3],embeddings[0])","metadata":{"execution":{"iopub.status.busy":"2022-08-06T09:52:41.128428Z","iopub.execute_input":"2022-08-06T09:52:41.128939Z","iopub.status.idle":"2022-08-06T09:53:15.328046Z","shell.execute_reply.started":"2022-08-06T09:52:41.128884Z","shell.execute_reply":"2022-08-06T09:53:15.327077Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"code","source":"from bert_score import score","metadata":{"execution":{"iopub.status.busy":"2022-08-06T09:53:15.329526Z","iopub.execute_input":"2022-08-06T09:53:15.330170Z","iopub.status.idle":"2022-08-06T09:53:15.335338Z","shell.execute_reply.started":"2022-08-06T09:53:15.330111Z","shell.execute_reply":"2022-08-06T09:53:15.334518Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2022-08-06T09:53:22.418682Z","iopub.execute_input":"2022-08-06T09:53:22.419067Z","iopub.status.idle":"2022-08-06T09:53:22.798191Z","shell.execute_reply.started":"2022-08-06T09:53:22.419036Z","shell.execute_reply":"2022-08-06T09:53:22.796111Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport matplotlib.pyplot as plt\n# select random image from validation data\nrid = np.random.randint(0, len(data))\nimage = data['images'][rid]\nreal_caption = data['text'][rid]\nprint(\"Real:\",image,real_caption)\n# caption,result,attention_weights = evaluate(image)\nimage = Image.open(root_dir + \"/\" + data['images'][rid]).convert(\"RGB\")\nplt.imshow(image)\nplt.show()\ncaption=predict(image)\ncaption=caption.replace(\"<s>\",\"\")\ncaption=caption.replace(\"</s>\",\"\")\ncaption=caption.rstrip()\nprint(\"Predicton\",caption.rstrip())","metadata":{"execution":{"iopub.status.busy":"2022-08-06T09:53:15.337297Z","iopub.execute_input":"2022-08-06T09:53:15.337698Z","iopub.status.idle":"2022-08-06T09:53:15.766033Z","shell.execute_reply.started":"2022-08-06T09:53:15.337661Z","shell.execute_reply":"2022-08-06T09:53:15.765148Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"from nltk.translate.bleu_score import corpus_bleu, sentence_bleu\nfrom nltk.translate.bleu_score import SmoothingFunction\nsmoothie = SmoothingFunction().method4\nscore = 0\nreferences=[]\nhypothesis=[]\n\nfor i in  (val_df['images'].unique().tolist()):\n\n    image = Image.open(root_dir + \"/\" + i).convert(\"RGB\")\n    caption = predict(image)\n    caption=caption.replace(\"<s>\",\"\")\n    caption=caption.replace(\"</s>\",\"\")\n    caption=caption.rstrip()\n    real = list(data[data['images']==i]['text'])\n#     print(real)\n    cleaned = []\n    for i in range(len(real)):\n        cleaned.append(real[i].split())\n    \n    references.append(cleaned)\n    hypothesis.append(caption.split())\n    \n    score += sentence_bleu(cleaned,caption.split(),smoothing_function=smoothie)\n\n\nval_bleu = score/len(val_df['images'].unique().tolist())*5\nprint(\"BLEU score on test data: \",val_bleu)\n# data\nbleu4 = corpus_bleu(references, hypothesis)#,weights=(0.25, 0.25, 0.25, 0.25),smoothing_function=SmoothingFunction().method1)\nbleu3 = corpus_bleu(references, hypothesis,weights=(0.33, 0.33, 0.33, 0.0))\nbleu2 = corpus_bleu(references, hypothesis,weights=(0.5, 0.5, 0.0, 0.0))\nbleu1 = corpus_bleu(references, hypothesis,weights=(1, 0.0, 0.0, 0.0))\nprint(bleu1,bleu2,bleu3,bleu4)","metadata":{"execution":{"iopub.status.busy":"2022-08-06T09:53:41.961919Z","iopub.execute_input":"2022-08-06T09:53:41.962308Z","iopub.status.idle":"2022-08-06T09:56:23.301526Z","shell.execute_reply.started":"2022-08-06T09:53:41.962274Z","shell.execute_reply":"2022-08-06T09:56:23.300134Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"bleu4 = corpus_bleu(references, hypothesis)#,weights=(0.25, 0.25, 0.25, 0.25),smoothing_function=SmoothingFunction().method1)\nbleu3 = corpus_bleu(references, hypothesis,weights=(0.33, 0.33, 0.33, 0.0))\nbleu2 = corpus_bleu(references, hypothesis,weights=(0.5, 0.5, 0.0, 0.0))\nbleu1 = corpus_bleu(references, hypothesis,weights=(1, 0.0, 0.0, 0.0))\nprint(bleu1,bleu2,bleu3,bleu4)","metadata":{"execution":{"iopub.status.busy":"2022-07-30T00:24:01.617725Z","iopub.execute_input":"2022-07-30T00:24:01.618100Z","iopub.status.idle":"2022-07-30T00:24:03.241182Z","shell.execute_reply.started":"2022-07-30T00:24:01.618072Z","shell.execute_reply":"2022-07-30T00:24:03.240332Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"code","source":"txt_hypothesis=[]\ntxt_references=[]\nfor txt in hypothesis:\n  txt_hypothesis.append(\" \".join(txt))\nfor data in references:\n  ref=[]\n  for txt in data:\n      ref.append(\" \".join(txt))\n  txt_references.append(ref)\ntxt_references[0:2]\n\nP, R, F1 = score(txt_hypothesis, txt_references, lang=\"ur\", verbose=False)\n# print( F1.mean())\nLoC=[]\nL_num=[]\nL_dnum=[]\nfor i,h in enumerate(txt_hypothesis):\n  LoC.append(h)\n  LoC.extend(txt_references[i])\nembeddings = laser.embed_sentences(LoC, lang='ur')\nfor i in range(0,embeddings.shape[0],6):\n  maxnum=0\n  maxdnum=1\n  for j in embeddings[i+1 : i+6]:\n    (num,dnum)=cosine_similarity(embeddings[i],j)\n    if dnum!=0.0 and num/dnum >maxnum/maxdnum:\n        (maxnum,maxdnum)=(num,dnum)\n  L_num.append(maxnum)\n  L_dnum.append(maxdnum)\nmicroLASER = np.sum( np.array(L_num) ) / np.sum( np.array(L_dnum) )\nmacroLASER = np.mean( np.array(L_num)/np.array(L_dnum) )\nprint(\"F1:\" ,F1.mean(),\"mircoLASER\",microLASER,'macroLaser',macroLASER)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nfrom nltk.translate.bleu_score import sentence_bleu\n# select random image from validation data\nrid = np.random.randint(0, len(data))\nimage = data['images'][rid]\nreal_caption = data['text'][rid]\nprint(\"Real:\",image,real_caption)\n# caption,result,attention_weights = evaluate(image)\nimage = Image.open(root_dir + \"/\" + data['images'][rid]).convert(\"RGB\")\nplt.imshow(image)\nplt.show()\ncaption=predict(image)\ncaption=caption.replace(\"<s>\",\"\")\ncaption=caption.replace(\"</s>\",\"\")\ncaption=caption.rstrip()\nprint(\"Prediction\",caption)\nfirst = real_caption\nreal_caption = first\n\n\n#remove \"<unk>\" in result\nfor i in caption:\n    if i==\"<unk>\":\n        caption.remove(i)\n\nfor i in real_caption:\n    if i==\"<unk>\":\n        real_caption.remove(i)\n\n\n\nreal_appn = []\nreal_appn.append(real_caption.split())\nreference = real_appn\ncandidate = caption.split()\nprint(reference,candidate)\nscore = sentence_bleu(reference, candidate, weights=(1.0,0,0,0))\nprint(f\"BLEU-1 score: {score*100}\")\nscore = sentence_bleu(reference, candidate, weights=(0.5,0.5,0,0))\nprint(f\"BLEU-2 score: {score*100}\")\nscore = sentence_bleu(reference, candidate, weights=(0.3,0.3,0.3,0))\nprint(f\"BLEU-3 score: {score*100}\")\nscore = sentence_bleu(reference, candidate, weights=(0.25,0.25,0.25,0.25))\nprint(f\"BLEU-4 score: {score*100}\")\n\n# print ('Real Caption:', real_caption)\n# print ('Predicted Caption:', ' '.join(caption))\n# temp_image = np.array(Image.open(image))\n# plt.imshow(temp_image)\n","metadata":{"execution":{"iopub.status.busy":"2022-07-30T00:24:27.028401Z","iopub.execute_input":"2022-07-30T00:24:27.028800Z","iopub.status.idle":"2022-07-30T00:24:27.469621Z","shell.execute_reply.started":"2022-07-30T00:24:27.028767Z","shell.execute_reply":"2022-07-30T00:24:27.468591Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"code","source":"import os\nos.environ[\"WANDB_DISABLED\"] = \"true\"\n","metadata":{"execution":{"iopub.status.busy":"2022-07-03T09:18:53.286011Z","iopub.execute_input":"2022-07-03T09:18:53.286788Z","iopub.status.idle":"2022-07-03T09:18:53.311307Z","shell.execute_reply.started":"2022-07-03T09:18:53.286753Z","shell.execute_reply":"2022-07-03T09:18:53.31059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# For comparison","metadata":{"execution":{"iopub.status.busy":"2022-07-30T01:00:53.119061Z","iopub.execute_input":"2022-07-30T01:00:53.119632Z","iopub.status.idle":"2022-07-30T01:00:53.126169Z","shell.execute_reply.started":"2022-07-30T01:00:53.119589Z","shell.execute_reply":"2022-07-30T01:00:53.125012Z"},"trusted":true},"execution_count":57,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nfrom nltk.translate.bleu_score import sentence_bleu\n# select random image from validation data\nrid = 35\nimage = data['images'][rid]\nreal_caption = data['text'][rid]\nprint(\"Real:\",image,real_caption)\n# caption,result,attention_weights = evaluate(image)\nimage = Image.open(root_dir + \"/1020651753_06077ec457.jpg\").convert(\"RGB\")\n# rid=img_name_val.index('/content/Flicker8k_Dataset/344078103_4b23931ce5.jpg')\nplt.imshow(image)\nplt.show()\ncaption=predict(image,beams=3)\ncaption=caption.replace(\"<s>\",\"\")\ncaption=caption.replace(\"</s>\",\"\")\ncaption=caption.rstrip()\nprint(\"Predicted\",caption)\nfirst = real_caption\nreal_caption = first\n\n\n#remove \"<unk>\" in result\nfor i in caption:\n    if i==\"<unk>\":\n        caption.remove(i)\n\nfor i in real_caption:\n    if i==\"<unk>\":\n        real_caption.remove(i)\n\n\n\nreal_appn = []\nreal_appn.append(real_caption.split())\nreference = real_appn\ncandidate = caption.split()\n# print(reference,candidate)\nscore = sentence_bleu(reference, candidate, weights=(1.0,0,0,0))\nprint(f\"BLEU-1 score: {score*100}\")\nscore = sentence_bleu(reference, candidate, weights=(0.5,0.5,0,0))\nprint(f\"BLEU-2 score: {score*100}\")\nscore = sentence_bleu(reference, candidate, weights=(0.3,0.3,0.3,0))\nprint(f\"BLEU-3 score: {score*100}\")\nscore = sentence_bleu(reference, candidate, weights=(0.25,0.25,0.25,0.25))\nprint(f\"BLEU-4 score: {score*100}\")\n\n# print ('Real Caption:', real_caption)\n# print ('Predicted Caption:', ' '.join(caption))\n# temp_image = np.array(Image.open(image))\n# plt.imshow(temp_image)\n","metadata":{"execution":{"iopub.status.busy":"2022-08-06T10:27:39.774947Z","iopub.execute_input":"2022-08-06T10:27:39.775322Z","iopub.status.idle":"2022-08-06T10:27:40.285819Z","shell.execute_reply.started":"2022-08-06T10:27:39.775289Z","shell.execute_reply":"2022-08-06T10:27:40.284924Z"},"trusted":true},"execution_count":71,"outputs":[]},{"cell_type":"code","source":"data[data['images']=='1020651753_06077ec457.jpg'].iloc[0]","metadata":{"execution":{"iopub.status.busy":"2022-08-06T10:27:25.284581Z","iopub.execute_input":"2022-08-06T10:27:25.285204Z","iopub.status.idle":"2022-08-06T10:27:25.294844Z","shell.execute_reply.started":"2022-08-06T10:27:25.285164Z","shell.execute_reply":"2022-08-06T10:27:25.293829Z"},"trusted":true},"execution_count":70,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nfrom nltk.translate.bleu_score import sentence_bleu\n# select random image from validation data\nrid = np.random.randint(0, len(data))\nimage = data['images'][rid]\nreal_caption = data['text'][rid]\nprint(\"Real:\",image,real_caption)\n# caption,result,attention_weights = evaluate(image)\nimage = Image.open(root_dir + \"/\" + data['images'][rid]).convert(\"RGB\")\nplt.imshow(image)\nplt.show()\ncaption=predict(image)\ncaption=caption.replace(\"<s>\",\"\")\ncaption=caption.replace(\"</s>\",\"\")\ncaption=caption.rstrip()\nprint(\"Predicted\",caption)\nfirst = real_caption\nreal_caption = first\n\n\n#remove \"<unk>\" in result\nfor i in caption:\n    if i==\"<unk>\":\n        caption.remove(i)\n\nfor i in real_caption:\n    if i==\"<unk>\":\n        real_caption.remove(i)\n\n\n\nreal_appn = []\nreal_appn.append(real_caption.split())\nreference = real_appn\ncandidate = caption.split()\n\nscore = sentence_bleu(reference, candidate, weights=(1.0,0,0,0))\nprint(f\"BLEU-1 score: {score*100}\")\nscore = sentence_bleu(reference, candidate, weights=(0.5,0.5,0,0))\nprint(f\"BLEU-2 score: {score*100}\")\nscore = sentence_bleu(reference, candidate, weights=(0.3,0.3,0.3,0))\nprint(f\"BLEU-3 score: {score*100}\")\nscore = sentence_bleu(reference, candidate, weights=(0.25,0.25,0.25,0.25))\nprint(f\"BLEU-4 score: {score*100}\")\nprint(reference,candidate)\n# print ('Real Caption:', real_caption)\n# print ('Predicted Caption:', ' '.join(caption))\n# temp_image = np.array(Image.open(image))\n# plt.imshow(temp_image)\n","metadata":{"execution":{"iopub.status.busy":"2022-07-30T00:50:22.470781Z","iopub.execute_input":"2022-07-30T00:50:22.471696Z","iopub.status.idle":"2022-07-30T00:50:22.916090Z","shell.execute_reply.started":"2022-07-30T00:50:22.471634Z","shell.execute_reply":"2022-07-30T00:50:22.915301Z"},"trusted":true},"execution_count":45,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nloss=[2.17,1.419,1.011,0.705,0.489,0.359200,0.28,0.23]\nval_loss=[1.48,0.98,0.62,0.402,0.290,0.22,0.19,0.16]\n\nplt.plot(loss,label=\"Train loss\")\nplt.plot(val_loss,label=\"Validaton Loss\")\nplt.legend(loc=\"upper right\")\n\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.title('Loss Plot')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-07-25T23:43:55.959702Z","iopub.execute_input":"2022-07-25T23:43:55.960125Z","iopub.status.idle":"2022-07-25T23:43:56.325370Z","shell.execute_reply.started":"2022-07-25T23:43:55.960092Z","shell.execute_reply":"2022-07-25T23:43:56.324621Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"bleu = evaluate.load(\"bleu\")\n      results = bleu.compute(predictions=predicted, references=references)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":" import matplotlib.pyplot as plt\nloss=[2.745300,1.837700,1.49080,1.224800,0.997500,0.765400,0.621100,0.512100]\nval_loss=[2.173391,2.021195,1.941288,1.931285,1.949279,1.986307,2.017984,2.058367]\n\nplt.plot(loss,label=\"Train loss\")\nplt.plot(val_loss,label=\"Validaton Loss\")\nplt.legend(loc=\"upper right\")\n\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.title('Loss Plot')\nplt.show()\n#     [8069/9000 2:11:57 < 15:13, 1.02 it/s, Epoch 8.96/10]\n# Epoch\tTraining Loss\tValidation Loss\n# 1\t2.745300\t2.173391\n# 2\t1.837700\t2.021195\n# 3\t1.490800\t1.941288\n# 4\t1.224800\t1.931285\n# 5\t0.997500\t1.949279\n# 6\t0.765400\t1.986307\n# 7\t0.621100\t2.017984\n# 8\t0.512100\t2.058367","metadata":{"execution":{"iopub.status.busy":"2022-08-06T10:06:10.101864Z","iopub.execute_input":"2022-08-06T10:06:10.102328Z","iopub.status.idle":"2022-08-06T10:06:10.310821Z","shell.execute_reply.started":"2022-08-06T10:06:10.102292Z","shell.execute_reply":"2022-08-06T10:06:10.310031Z"},"trusted":true},"execution_count":32,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}